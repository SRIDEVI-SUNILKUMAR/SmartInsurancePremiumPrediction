{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77783863-1cb4-49d3-afc6-82ecd38b7953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlflow\n",
      "  Downloading mlflow-3.1.1-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting mlflow-skinny==3.1.1 (from mlflow)\n",
      "  Downloading mlflow_skinny-3.1.1-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: Flask<4 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from mlflow) (3.0.3)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from mlflow) (1.13.3)\n",
      "Collecting docker<8,>=4.0.0 (from mlflow)\n",
      "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting graphene<4 (from mlflow)\n",
      "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: matplotlib<4 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from mlflow) (3.9.2)\n",
      "Requirement already satisfied: numpy<3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from mlflow) (1.26.4)\n",
      "Requirement already satisfied: pandas<3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from mlflow) (2.2.2)\n",
      "Requirement already satisfied: pyarrow<21,>=4.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from mlflow) (16.1.0)\n",
      "Requirement already satisfied: scikit-learn<2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from mlflow) (1.5.1)\n",
      "Requirement already satisfied: scipy<2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from mlflow) (1.13.1)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from mlflow) (2.0.34)\n",
      "Collecting waitress<4 (from mlflow)\n",
      "  Downloading waitress-3.0.2-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: cachetools<7,>=5.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.1.1->mlflow) (5.3.3)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.1.1->mlflow) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle<4 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.1.1->mlflow) (3.0.0)\n",
      "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==3.1.1->mlflow)\n",
      "  Downloading databricks_sdk-0.59.0-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting fastapi<1 (from mlflow-skinny==3.1.1->mlflow)\n",
      "  Downloading fastapi-0.116.1-py3-none-any.whl.metadata (28 kB)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.1.1->mlflow) (3.1.43)\n",
      "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.1.1->mlflow) (7.0.1)\n",
      "Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==3.1.1->mlflow)\n",
      "  Downloading opentelemetry_api-1.35.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==3.1.1->mlflow)\n",
      "  Downloading opentelemetry_sdk-1.35.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: packaging<26 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.1.1->mlflow) (24.1)\n",
      "Requirement already satisfied: protobuf<7,>=3.12.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.1.1->mlflow) (4.25.3)\n",
      "Requirement already satisfied: pydantic<3,>=1.10.8 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.1.1->mlflow) (2.8.2)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.1.1->mlflow) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.1.1->mlflow) (2.32.3)\n",
      "Collecting sqlparse<1,>=0.4.0 (from mlflow-skinny==3.1.1->mlflow)\n",
      "  Downloading sqlparse-0.5.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from mlflow-skinny==3.1.1->mlflow) (4.13.2)\n",
      "Collecting uvicorn<1 (from mlflow-skinny==3.1.1->mlflow)\n",
      "  Downloading uvicorn-0.35.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: Mako in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from alembic!=1.10.0,<2->mlflow) (1.2.3)\n",
      "Requirement already satisfied: pywin32>=304 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from docker<8,>=4.0.0->mlflow) (305.1)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from docker<8,>=4.0.0->mlflow) (2.4.0)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from Flask<4->mlflow) (3.0.3)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from Flask<4->mlflow) (3.1.4)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from Flask<4->mlflow) (2.2.0)\n",
      "Requirement already satisfied: blinker>=1.6.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from Flask<4->mlflow) (1.6.2)\n",
      "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
      "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
      "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from graphene<4->mlflow) (2.9.0.post0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow) (1.4.4)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow) (3.1.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pandas<3->mlflow) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pandas<3->mlflow) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from scikit-learn<2->mlflow) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from scikit-learn<2->mlflow) (3.5.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from click<9,>=7.0->mlflow-skinny==3.1.1->mlflow) (0.4.6)\n",
      "Collecting google-auth~=2.0 (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow)\n",
      "  Downloading google_auth-2.40.3-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting starlette<0.48.0,>=0.40.0 (from fastapi<1->mlflow-skinny==3.1.1->mlflow)\n",
      "  Downloading starlette-0.47.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.1.1->mlflow) (4.0.7)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.1.1->mlflow) (3.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from Jinja2>=3.1.2->Flask<4->mlflow) (2.1.3)\n",
      "Collecting opentelemetry-semantic-conventions==0.56b0 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.1.1->mlflow)\n",
      "  Downloading opentelemetry_semantic_conventions-0.56b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.1->mlflow) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.1->mlflow) (2.20.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.1->mlflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.1->mlflow) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.1->mlflow) (2025.6.15)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from uvicorn<1->mlflow-skinny==3.1.1->mlflow) (0.14.0)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.1.1->mlflow) (4.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from starlette<0.48.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.1->mlflow) (4.2.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi<1->mlflow-skinny==3.1.1->mlflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.1->mlflow) (0.4.8)\n",
      "Downloading mlflow-3.1.1-py3-none-any.whl (24.7 MB)\n",
      "   ---------------------------------------- 0.0/24.7 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.6/24.7 MB 7.6 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 3.1/24.7 MB 7.4 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 4.7/24.7 MB 7.3 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 6.3/24.7 MB 7.4 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 7.1/24.7 MB 7.4 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 9.4/24.7 MB 7.3 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 11.0/24.7 MB 7.3 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 12.6/24.7 MB 7.3 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 13.9/24.7 MB 7.3 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 15.5/24.7 MB 7.3 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 17.0/24.7 MB 7.3 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 18.9/24.7 MB 7.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 20.2/24.7 MB 7.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 21.8/24.7 MB 7.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.3/24.7 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/24.7 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/24.7 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.7/24.7 MB 6.6 MB/s eta 0:00:00\n",
      "Downloading mlflow_skinny-3.1.1-py3-none-any.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   -------------------------------- ------- 1.6/1.9 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 6.3 MB/s eta 0:00:00\n",
      "Downloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "Downloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
      "Downloading waitress-3.0.2-py3-none-any.whl (56 kB)\n",
      "Downloading databricks_sdk-0.59.0-py3-none-any.whl (676 kB)\n",
      "   ---------------------------------------- 0.0/676.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 676.2/676.2 kB 5.3 MB/s eta 0:00:00\n",
      "Downloading fastapi-0.116.1-py3-none-any.whl (95 kB)\n",
      "Downloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
      "Downloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading opentelemetry_api-1.35.0-py3-none-any.whl (65 kB)\n",
      "Downloading opentelemetry_sdk-1.35.0-py3-none-any.whl (119 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.56b0-py3-none-any.whl (201 kB)\n",
      "Downloading sqlparse-0.5.3-py3-none-any.whl (44 kB)\n",
      "Downloading uvicorn-0.35.0-py3-none-any.whl (66 kB)\n",
      "Downloading google_auth-2.40.3-py2.py3-none-any.whl (216 kB)\n",
      "Downloading starlette-0.47.1-py3-none-any.whl (72 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Installing collected packages: waitress, sqlparse, rsa, graphql-core, uvicorn, starlette, opentelemetry-api, graphql-relay, google-auth, docker, opentelemetry-semantic-conventions, graphene, fastapi, databricks-sdk, opentelemetry-sdk, mlflow-skinny, mlflow\n",
      "Successfully installed databricks-sdk-0.59.0 docker-7.1.0 fastapi-0.116.1 google-auth-2.40.3 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 mlflow-3.1.1 mlflow-skinny-3.1.1 opentelemetry-api-1.35.0 opentelemetry-sdk-1.35.0 opentelemetry-semantic-conventions-0.56b0 rsa-4.9.1 sqlparse-0.5.3 starlette-0.47.1 uvicorn-0.35.0 waitress-3.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b64b8bc8-c1b3-497b-9236-5247258952a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/27 10:59:26 INFO mlflow.tracking.fluent: Experiment with name 'InsurancePremiumPrediction_Linear' does not exist. Creating a new experiment.\n",
      "2025/07/27 10:59:41 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/07/27 11:00:02 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Linear Regression Evaluation Results:\n",
      "RMSE: 940.14\n",
      "MAE: 648.04\n",
      "R² Score: -0.1828\n",
      "RMSLE: 1.0892\n",
      "📄 submission.csv with Linear Regression predictions saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 📘 Insurance Premium Prediction – Linear Regression Version\n",
    "# ================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# 1. Load data\n",
    "df = pd.read_csv(\"C:/Users/Lenovo/Documents/Guvi/Project_3/train.csv\")\n",
    "df = df.drop(columns=[\"Policy Start Date\", \"Customer Feedback\"])\n",
    "\n",
    "# 2. Define features\n",
    "numerical_cols = [\n",
    "    \"Age\", \"Annual Income\", \"Number of Dependents\", \"Health Score\",\n",
    "    \"Previous Claims\", \"Vehicle Age\", \"Credit Score\", \"Insurance Duration\"\n",
    "]\n",
    "\n",
    "categorical_cols = [\n",
    "    \"Gender\", \"Marital Status\", \"Education Level\", \"Occupation\",\n",
    "    \"Location\", \"Policy Type\", \"Smoking Status\",\n",
    "    \"Exercise Frequency\", \"Property Type\"\n",
    "]\n",
    "\n",
    "# 3. Add simple feature engineering\n",
    "df[\"Income_per_Dependent\"] = df[\"Annual Income\"] / (df[\"Number of Dependents\"] + 1)\n",
    "df[\"Claims_per_Year\"] = df[\"Previous Claims\"] / df[\"Insurance Duration\"].replace(0, 1)\n",
    "numerical_cols += [\"Income_per_Dependent\", \"Claims_per_Year\"]\n",
    "\n",
    "# 4. Prepare target (log transform)\n",
    "target_col = \"Premium Amount\"\n",
    "X = df[numerical_cols + categorical_cols]\n",
    "y = np.log1p(df[target_col])\n",
    "\n",
    "# 5. Preprocessing\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"num\", numerical_transformer, numerical_cols),\n",
    "    (\"cat\", categorical_transformer, categorical_cols)\n",
    "])\n",
    "\n",
    "# 6. Train/test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 7. Linear regression pipeline\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", LinearRegression())\n",
    "])\n",
    "\n",
    "# 8. MLflow experiment\n",
    "mlflow.set_experiment(\"InsurancePremiumPrediction_Linear\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "\n",
    "    # Fit model\n",
    "    model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_pred_log = model_pipeline.predict(X_val)\n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "    y_val_original = np.expm1(y_val)\n",
    "\n",
    "    # Metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_val_original, y_pred))\n",
    "    mae = mean_absolute_error(y_val_original, y_pred)\n",
    "    r2 = r2_score(y_val_original, y_pred)\n",
    "    rmsle = np.sqrt(mean_squared_error(np.log1p(y_val_original), np.log1p(np.maximum(y_pred, 0))))\n",
    "\n",
    "    # Log results\n",
    "    mlflow.log_param(\"model_type\", \"LinearRegression\")\n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "    mlflow.log_metric(\"MAE\", mae)\n",
    "    mlflow.log_metric(\"R2\", r2)\n",
    "    mlflow.log_metric(\"RMSLE\", rmsle)\n",
    "\n",
    "    # Save model\n",
    "    joblib.dump(model_pipeline, \"linear_model.pkl\")\n",
    "    mlflow.sklearn.log_model(model_pipeline, \"linear_model\")\n",
    "\n",
    "# 9. Output metrics\n",
    "print(\"✅ Linear Regression Evaluation Results:\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(f\"RMSLE: {rmsle:.4f}\")\n",
    "\n",
    "# ===============================\n",
    "# 📦 Predict on Unlabeled Test Set\n",
    "# ===============================\n",
    "test_unlabeled = pd.read_csv(\"C:/Users/Lenovo/Documents/Guvi/Project_3/test.csv\")\n",
    "test_unlabeled = test_unlabeled.drop(columns=[\"Policy Start Date\", \"Customer Feedback\"])\n",
    "\n",
    "# Add features\n",
    "test_unlabeled[\"Income_per_Dependent\"] = test_unlabeled[\"Annual Income\"] / (test_unlabeled[\"Number of Dependents\"] + 1)\n",
    "test_unlabeled[\"Claims_per_Year\"] = test_unlabeled[\"Previous Claims\"] / test_unlabeled[\"Insurance Duration\"].replace(0, 1)\n",
    "\n",
    "# Predict\n",
    "X_unlabeled = test_unlabeled[numerical_cols + categorical_cols]\n",
    "test_log_preds = model_pipeline.predict(X_unlabeled)\n",
    "test_preds = np.expm1(test_log_preds)\n",
    "\n",
    "# Save predictions\n",
    "submission = pd.DataFrame({\n",
    "    \"Customer_ID\": test_unlabeled.get(\"Customer_ID\", range(len(test_unlabeled))),\n",
    "    \"Predicted_Premium_Amount\": test_preds\n",
    "})\n",
    "submission.to_csv(\"C:/Users/Lenovo/Documents/Guvi/Project_3/submission_Linear_regression.csv\", index=False)\n",
    "\n",
    "print(\"📄 submission.csv with Linear Regression predictions saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909738c5-d7da-4eca-91ac-ef0aeb42b0e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff631674-6f7d-47f6-afd9-3938291528c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.6.0-py3-none-win_amd64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from lightgbm) (1.13.1)\n",
      "Downloading lightgbm-4.6.0-py3-none-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------------------------------------ --- 1.3/1.5 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 6.4 MB/s eta 0:00:00\n",
      "Installing collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eaca65-0b40-4492-949d-d613835943e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f85d9a5f-4d1e-4ba6-bef3-73147e323aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.033353 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 913\n",
      "[LightGBM] [Info] Number of data points in the train set: 960000, number of used features: 35\n",
      "[LightGBM] [Info] Start training from score 1102.505529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/20 15:25:52 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/07/20 15:26:26 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model Evaluation Results:\n",
      "RMSE: 847.25\n",
      "MAE: 646.20\n",
      "R² Score: 0.0394\n",
      "RMSLE: 1.1493\n",
      "📄 submission.csv with predictions saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# 📘 Insurance Premium Prediction ML Pipeline\n",
    "# =============================================\n",
    "\n",
    "# 1️⃣ Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# 2️⃣ Load the training dataset\n",
    "# Make sure this file contains the target column 'Premium Amount'\n",
    "df = pd.read_csv(\"C:/Users/Lenovo/Documents/Guvi/Project_3/train.csv\")\n",
    "\n",
    "# 3️⃣ Drop non-numeric/non-informative columns for now\n",
    "# 'Policy Start Date' is text and poorly formatted\n",
    "# 'Customer Feedback' is unstructured text; needs NLP (optional)\n",
    "df = df.drop(columns=[\"Policy Start Date\", \"Customer Feedback\"])\n",
    "\n",
    "# 4️⃣ Define feature columns\n",
    "# These are manually selected based on your dataset description\n",
    "\n",
    "numerical_cols = [\n",
    "    \"Age\", \"Annual Income\", \"Number of Dependents\", \"Health Score\",\n",
    "    \"Previous Claims\", \"Vehicle Age\", \"Credit Score\", \"Insurance Duration\"\n",
    "]\n",
    "\n",
    "categorical_cols = [\n",
    "    \"Gender\", \"Marital Status\", \"Education Level\", \"Occupation\",\n",
    "    \"Location\", \"Policy Type\", \"Smoking Status\",\n",
    "    \"Exercise Frequency\", \"Property Type\"\n",
    "]\n",
    "\n",
    "target_col = \"Premium Amount\"\n",
    "\n",
    "# 5️⃣ Separate features (X) and target (y)\n",
    "X = df[numerical_cols + categorical_cols]\n",
    "y = df[target_col]\n",
    "\n",
    "# 6️⃣ Preprocessing Pipelines\n",
    "# Numerical pipeline: Fill missing values with mean and standardize\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical pipeline: Fill missing with most frequent and encode\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "# Combine both into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"num\", numerical_transformer, numerical_cols),\n",
    "    (\"cat\", categorical_transformer, categorical_cols)\n",
    "])\n",
    "\n",
    "# 7️⃣ Split into training and validation sets (80/20)\n",
    "# This helps us test how the model performs on unseen data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 8️⃣ Create a complete ML pipeline with preprocessing + model\n",
    "# Replaced RandomForest with LightGBM in the pipeline\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", LGBMRegressor(n_estimators=100, learning_rate=0.1, random_state=42))\n",
    "])\n",
    "\n",
    "# 9️⃣ Start MLflow to track experiment\n",
    "mlflow.set_experiment(\"InsurancePremiumPrediction\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "\n",
    "    # Train the model\n",
    "    model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on validation set\n",
    "    y_pred = model_pipeline.predict(X_val)\n",
    "\n",
    "    # 10️⃣ Calculate Evaluation Metrics\n",
    "\n",
    "    # Root Mean Squared Error\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "\n",
    "    # Mean Absolute Error\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "\n",
    "    # R² Score (how much variance is explained)\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "    # RMSLE – Root Mean Squared Log Error\n",
    "    # Useful for skewed targets like insurance costs\n",
    "    rmsle = np.sqrt(mean_squared_error(np.log1p(y_val), np.log1p(y_pred)))\n",
    "\n",
    "    # 11️⃣ Log all metrics and parameters to MLflow\n",
    "    mlflow.log_param(\"model_type\", \"RandomForest\")\n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "    mlflow.log_metric(\"MAE\", mae)\n",
    "    mlflow.log_metric(\"R2\", r2)\n",
    "    mlflow.log_metric(\"RMSLE\", rmsle)\n",
    "\n",
    "    # 12️⃣ Save model locally and log to MLflow\n",
    "    joblib.dump(model_pipeline, \"trained_model.pkl\")  # Local file\n",
    "    mlflow.sklearn.log_model(model_pipeline, \"model\")  # MLflow UI\n",
    "\n",
    "# 13️⃣ Final evaluation output\n",
    "print(\"✅ Model Evaluation Results:\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(f\"RMSLE: {rmsle:.4f}\")\n",
    "\n",
    "\n",
    "# ========================================\n",
    "# 📦 14️⃣ Predict on Unlabeled Test Set\n",
    "# ========================================\n",
    "\n",
    "# Load the test data (no Premium Amount column)\n",
    "test_unlabeled = pd.read_csv(\"C:/Users/Lenovo/Documents/Guvi/Project_3/test.csv\")\n",
    "\n",
    "# Drop unused/unstructured columns\n",
    "test_unlabeled = test_unlabeled.drop(columns=[\"Policy Start Date\", \"Customer Feedback\"])\n",
    "\n",
    "# Select features only\n",
    "X_unlabeled = test_unlabeled[numerical_cols + categorical_cols]\n",
    "\n",
    "# Use trained pipeline to predict premium amounts\n",
    "test_predictions = model_pipeline.predict(X_unlabeled)\n",
    "\n",
    "# Create a new DataFrame with predictions\n",
    "submission = pd.DataFrame({\n",
    "    \"Customer_ID\": test_unlabeled.get(\"Customer_ID\", range(len(test_unlabeled))),\n",
    "    \"Predicted_Premium_Amount\": test_predictions\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "submission.to_csv(\"C:/Users/Lenovo/Documents/Guvi/Project_3/submission_LGBM.csv\", index=False)\n",
    "\n",
    "print(\"📄 submission.csv with predictions saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c7ee0b7-7f1a-4a57-81d2-3f765c136647",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/27 11:01:21 INFO mlflow.tracking.fluent: Experiment with name 'InsurancePremiumPrediction_DecisionTree' does not exist. Creating a new experiment.\n",
      "2025/07/27 11:01:47 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/07/27 11:01:57 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Decision Tree Evaluation Results:\n",
      "RMSE: 927.66\n",
      "MAE: 625.68\n",
      "R² Score: -0.1516\n",
      "RMSLE: 1.0605\n",
      "📄 submission.csv with Decision Tree predictions saved successfully!\n"
     ]
    }
   ],
   "source": [
    " # ================================\n",
    "# 📘 Insurance Premium Prediction – Decision Tree Version\n",
    "# ================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# 1. Load data\n",
    "df = pd.read_csv(\"C:/Users/Lenovo/Documents/Guvi/Project_3/train.csv\")\n",
    "df = df.drop(columns=[\"Policy Start Date\", \"Customer Feedback\"])\n",
    "\n",
    "# 2. Define features\n",
    "numerical_cols = [\n",
    "    \"Age\", \"Annual Income\", \"Number of Dependents\", \"Health Score\",\n",
    "    \"Previous Claims\", \"Vehicle Age\", \"Credit Score\", \"Insurance Duration\"\n",
    "]\n",
    "\n",
    "categorical_cols = [\n",
    "    \"Gender\", \"Marital Status\", \"Education Level\", \"Occupation\",\n",
    "    \"Location\", \"Policy Type\", \"Smoking Status\",\n",
    "    \"Exercise Frequency\", \"Property Type\"\n",
    "]\n",
    "\n",
    "# 3. Add feature engineering\n",
    "df[\"Income_per_Dependent\"] = df[\"Annual Income\"] / (df[\"Number of Dependents\"] + 1)\n",
    "df[\"Claims_per_Year\"] = df[\"Previous Claims\"] / df[\"Insurance Duration\"].replace(0, 1)\n",
    "numerical_cols += [\"Income_per_Dependent\", \"Claims_per_Year\"]\n",
    "\n",
    "# 4. Target variable with log transform\n",
    "target_col = \"Premium Amount\"\n",
    "X = df[numerical_cols + categorical_cols]\n",
    "y = np.log1p(df[target_col])\n",
    "\n",
    "# 5. Preprocessing pipelines\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"num\", numerical_transformer, numerical_cols),\n",
    "    (\"cat\", categorical_transformer, categorical_cols)\n",
    "])\n",
    "\n",
    "# 6. Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 7. Build pipeline with Decision Tree\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", DecisionTreeRegressor(max_depth=6, random_state=42))\n",
    "])\n",
    "\n",
    "# 8. Start MLflow experiment\n",
    "mlflow.set_experiment(\"InsurancePremiumPrediction_DecisionTree\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "\n",
    "    # Train model\n",
    "    model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_pred_log = model_pipeline.predict(X_val)\n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "    y_val_original = np.expm1(y_val)\n",
    "\n",
    "    # Metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_val_original, y_pred))\n",
    "    mae = mean_absolute_error(y_val_original, y_pred)\n",
    "    r2 = r2_score(y_val_original, y_pred)\n",
    "    rmsle = np.sqrt(mean_squared_error(np.log1p(y_val_original), np.log1p(np.maximum(y_pred, 0))))\n",
    "\n",
    "    # Log to MLflow\n",
    "    mlflow.log_param(\"model_type\", \"DecisionTree\")\n",
    "    mlflow.log_param(\"max_depth\", 6)\n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "    mlflow.log_metric(\"MAE\", mae)\n",
    "    mlflow.log_metric(\"R2\", r2)\n",
    "    mlflow.log_metric(\"RMSLE\", rmsle)\n",
    "\n",
    "    # Save model\n",
    "    joblib.dump(model_pipeline, \"decision_tree_model.pkl\")\n",
    "    mlflow.sklearn.log_model(model_pipeline, \"decision_tree_model\")\n",
    "\n",
    "# 9. Output metrics\n",
    "print(\"✅ Decision Tree Evaluation Results:\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(f\"RMSLE: {rmsle:.4f}\")\n",
    "\n",
    "# ===============================\n",
    "# 📦 Predict on Unlabeled Test Set\n",
    "# ===============================\n",
    "test_unlabeled = pd.read_csv(\"C:/Users/Lenovo/Documents/Guvi/Project_3/test.csv\")\n",
    "test_unlabeled = test_unlabeled.drop(columns=[\"Policy Start Date\", \"Customer Feedback\"])\n",
    "\n",
    "# Add engineered features\n",
    "test_unlabeled[\"Income_per_Dependent\"] = test_unlabeled[\"Annual Income\"] / (test_unlabeled[\"Number of Dependents\"] + 1)\n",
    "test_unlabeled[\"Claims_per_Year\"] = test_unlabeled[\"Previous Claims\"] / test_unlabeled[\"Insurance Duration\"].replace(0, 1)\n",
    "\n",
    "X_unlabeled = test_unlabeled[numerical_cols + categorical_cols]\n",
    "test_log_preds = model_pipeline.predict(X_unlabeled)\n",
    "test_preds = np.expm1(test_log_preds)\n",
    "\n",
    "# Save predictions\n",
    "submission = pd.DataFrame({\n",
    "    \"Customer_ID\": test_unlabeled.get(\"Customer_ID\", range(len(test_unlabeled))),\n",
    "    \"Predicted_Premium_Amount\": test_preds\n",
    "})\n",
    "submission.to_csv(\"C:/Users/Lenovo/Documents/Guvi/Project_3/submission_decision_tree.csv\", index=False)\n",
    "\n",
    "print(\"📄 submission.csv with Decision Tree predictions saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51e9d687-cc8f-4187-8fd3-00b5bfe569a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/27 11:02:39 INFO mlflow.tracking.fluent: Experiment with name 'InsurancePremiumPrediction_RandomForest' does not exist. Creating a new experiment.\n",
      "2025/07/27 11:10:24 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/07/27 11:10:36 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Random Forest Evaluation Results:\n",
      "RMSE: 927.45\n",
      "MAE: 623.54\n",
      "R² Score: -0.1511\n",
      "RMSLE: 1.0563\n",
      "📄 submission.csv with Random Forest predictions saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 📘 Insurance Premium Prediction – Random Forest Version\n",
    "# ================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# 1. Load training dataset\n",
    "df = pd.read_csv(\"C:/Users/Lenovo/Documents/Guvi/Project_3/train.csv\")\n",
    "df = df.drop(columns=[\"Policy Start Date\", \"Customer Feedback\"])\n",
    "\n",
    "# 2. Define feature columns\n",
    "numerical_cols = [\n",
    "    \"Age\", \"Annual Income\", \"Number of Dependents\", \"Health Score\",\n",
    "    \"Previous Claims\", \"Vehicle Age\", \"Credit Score\", \"Insurance Duration\"\n",
    "]\n",
    "\n",
    "categorical_cols = [\n",
    "    \"Gender\", \"Marital Status\", \"Education Level\", \"Occupation\",\n",
    "    \"Location\", \"Policy Type\", \"Smoking Status\",\n",
    "    \"Exercise Frequency\", \"Property Type\"\n",
    "]\n",
    "\n",
    "# 3. Feature engineering\n",
    "df[\"Income_per_Dependent\"] = df[\"Annual Income\"] / (df[\"Number of Dependents\"] + 1)\n",
    "df[\"Claims_per_Year\"] = df[\"Previous Claims\"] / df[\"Insurance Duration\"].replace(0, 1)\n",
    "numerical_cols += [\"Income_per_Dependent\", \"Claims_per_Year\"]\n",
    "\n",
    "# 4. Target and features\n",
    "target_col = \"Premium Amount\"\n",
    "X = df[numerical_cols + categorical_cols]\n",
    "y = np.log1p(df[target_col])  # log transform to reduce skew\n",
    "\n",
    "# 5. Preprocessing pipelines\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"num\", numerical_transformer, numerical_cols),\n",
    "    (\"cat\", categorical_transformer, categorical_cols)\n",
    "])\n",
    "\n",
    "# 6. Train/test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 7. Random Forest pipeline\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 8. MLflow experiment\n",
    "mlflow.set_experiment(\"InsurancePremiumPrediction_RandomForest\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "\n",
    "    # Fit model\n",
    "    model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_pred_log = model_pipeline.predict(X_val)\n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "    y_val_original = np.expm1(y_val)\n",
    "\n",
    "    # Evaluation metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_val_original, y_pred))\n",
    "    mae = mean_absolute_error(y_val_original, y_pred)\n",
    "    r2 = r2_score(y_val_original, y_pred)\n",
    "    rmsle = np.sqrt(mean_squared_error(np.log1p(y_val_original), np.log1p(np.maximum(y_pred, 0))))\n",
    "\n",
    "    # Log to MLflow\n",
    "    mlflow.log_param(\"model_type\", \"RandomForest\")\n",
    "    mlflow.log_param(\"n_estimators\", 100)\n",
    "    mlflow.log_param(\"max_depth\", 10)\n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "    mlflow.log_metric(\"MAE\", mae)\n",
    "    mlflow.log_metric(\"R2\", r2)\n",
    "    mlflow.log_metric(\"RMSLE\", rmsle)\n",
    "\n",
    "    # Save model\n",
    "    joblib.dump(model_pipeline, \"random_forest_model.pkl\")\n",
    "    mlflow.sklearn.log_model(model_pipeline, \"random_forest_model\")\n",
    "\n",
    "# 9. Print results\n",
    "print(\"✅ Random Forest Evaluation Results:\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(f\"RMSLE: {rmsle:.4f}\")\n",
    "\n",
    "# ===============================\n",
    "# 📦 Predict on Unlabeled Test Set\n",
    "# ===============================\n",
    "test_unlabeled = pd.read_csv(\"C:/Users/Lenovo/Documents/Guvi/Project_3/test.csv\")\n",
    "test_unlabeled = test_unlabeled.drop(columns=[\"Policy Start Date\", \"Customer Feedback\"])\n",
    "\n",
    "# Add same engineered features\n",
    "test_unlabeled[\"Income_per_Dependent\"] = test_unlabeled[\"Annual Income\"] / (test_unlabeled[\"Number of Dependents\"] + 1)\n",
    "test_unlabeled[\"Claims_per_Year\"] = test_unlabeled[\"Previous Claims\"] / test_unlabeled[\"Insurance Duration\"].replace(0, 1)\n",
    "\n",
    "X_unlabeled = test_unlabeled[numerical_cols + categorical_cols]\n",
    "test_log_preds = model_pipeline.predict(X_unlabeled)\n",
    "test_preds = np.expm1(test_log_preds)\n",
    "\n",
    "# Save predictions to CSV\n",
    "submission = pd.DataFrame({\n",
    "    \"Customer_ID\": test_unlabeled.get(\"Customer_ID\", range(len(test_unlabeled))),\n",
    "    \"Predicted_Premium_Amount\": test_preds\n",
    "})\n",
    "submission.to_csv(\"C:/Users/Lenovo/Documents/Guvi/Project_3/submission_Random_forest.csv\", index=False)\n",
    "\n",
    "print(\"📄 submission.csv with Random Forest predictions saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbc79734-6564-486c-8ed9-34d63fdbff67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-3.0.2-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Downloading xgboost-3.0.2-py3-none-win_amd64.whl (150.0 MB)\n",
      "   ---------------------------------------- 0.0/150.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.3/150.0 MB 8.4 MB/s eta 0:00:18\n",
      "    --------------------------------------- 2.9/150.0 MB 7.6 MB/s eta 0:00:20\n",
      "   - -------------------------------------- 4.5/150.0 MB 7.4 MB/s eta 0:00:20\n",
      "   - -------------------------------------- 6.3/150.0 MB 7.4 MB/s eta 0:00:20\n",
      "   -- ------------------------------------- 7.6/150.0 MB 7.3 MB/s eta 0:00:20\n",
      "   -- ------------------------------------- 9.2/150.0 MB 7.3 MB/s eta 0:00:20\n",
      "   -- ------------------------------------- 10.7/150.0 MB 7.4 MB/s eta 0:00:19\n",
      "   --- ------------------------------------ 12.3/150.0 MB 7.3 MB/s eta 0:00:19\n",
      "   --- ------------------------------------ 13.9/150.0 MB 7.3 MB/s eta 0:00:19\n",
      "   ---- ----------------------------------- 15.5/150.0 MB 7.3 MB/s eta 0:00:19\n",
      "   ---- ----------------------------------- 17.0/150.0 MB 7.3 MB/s eta 0:00:19\n",
      "   ---- ----------------------------------- 18.6/150.0 MB 7.3 MB/s eta 0:00:19\n",
      "   ----- ---------------------------------- 20.2/150.0 MB 7.3 MB/s eta 0:00:18\n",
      "   ----- ---------------------------------- 21.5/150.0 MB 7.3 MB/s eta 0:00:18\n",
      "   ------ --------------------------------- 23.1/150.0 MB 7.3 MB/s eta 0:00:18\n",
      "   ------ --------------------------------- 24.6/150.0 MB 7.3 MB/s eta 0:00:18\n",
      "   ------ --------------------------------- 26.2/150.0 MB 7.3 MB/s eta 0:00:17\n",
      "   ------- -------------------------------- 27.8/150.0 MB 7.3 MB/s eta 0:00:17\n",
      "   ------- -------------------------------- 29.4/150.0 MB 7.3 MB/s eta 0:00:17\n",
      "   -------- ------------------------------- 30.9/150.0 MB 7.3 MB/s eta 0:00:17\n",
      "   -------- ------------------------------- 32.5/150.0 MB 7.3 MB/s eta 0:00:17\n",
      "   --------- ------------------------------ 34.1/150.0 MB 7.3 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 35.7/150.0 MB 7.3 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 37.2/150.0 MB 7.3 MB/s eta 0:00:16\n",
      "   ---------- ----------------------------- 38.8/150.0 MB 7.3 MB/s eta 0:00:16\n",
      "   ---------- ----------------------------- 40.4/150.0 MB 7.3 MB/s eta 0:00:16\n",
      "   ----------- ---------------------------- 41.9/150.0 MB 7.3 MB/s eta 0:00:15\n",
      "   ----------- ---------------------------- 43.5/150.0 MB 7.3 MB/s eta 0:00:15\n",
      "   ------------ --------------------------- 45.1/150.0 MB 7.3 MB/s eta 0:00:15\n",
      "   ------------ --------------------------- 46.7/150.0 MB 7.3 MB/s eta 0:00:15\n",
      "   ------------ --------------------------- 48.2/150.0 MB 7.3 MB/s eta 0:00:14\n",
      "   ------------- -------------------------- 49.5/150.0 MB 7.3 MB/s eta 0:00:14\n",
      "   ------------- -------------------------- 51.1/150.0 MB 7.3 MB/s eta 0:00:14\n",
      "   -------------- ------------------------- 52.7/150.0 MB 7.3 MB/s eta 0:00:14\n",
      "   -------------- ------------------------- 54.3/150.0 MB 7.3 MB/s eta 0:00:14\n",
      "   -------------- ------------------------- 55.8/150.0 MB 7.3 MB/s eta 0:00:13\n",
      "   --------------- ------------------------ 57.4/150.0 MB 7.3 MB/s eta 0:00:13\n",
      "   --------------- ------------------------ 58.7/150.0 MB 7.3 MB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 60.3/150.0 MB 7.3 MB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 61.9/150.0 MB 7.3 MB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 63.4/150.0 MB 7.3 MB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 65.0/150.0 MB 7.3 MB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 65.8/150.0 MB 7.3 MB/s eta 0:00:12\n",
      "   ------------------ --------------------- 67.9/150.0 MB 7.2 MB/s eta 0:00:12\n",
      "   ------------------ --------------------- 69.2/150.0 MB 7.2 MB/s eta 0:00:12\n",
      "   ------------------ --------------------- 70.8/150.0 MB 7.2 MB/s eta 0:00:11\n",
      "   ------------------- -------------------- 72.1/150.0 MB 7.2 MB/s eta 0:00:11\n",
      "   ------------------- -------------------- 73.7/150.0 MB 7.2 MB/s eta 0:00:11\n",
      "   ------------------- -------------------- 75.0/150.0 MB 7.2 MB/s eta 0:00:11\n",
      "   -------------------- ------------------- 76.5/150.0 MB 7.2 MB/s eta 0:00:11\n",
      "   -------------------- ------------------- 78.1/150.0 MB 7.2 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 79.7/150.0 MB 7.2 MB/s eta 0:00:10\n",
      "   --------------------- ------------------ 81.0/150.0 MB 7.2 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 82.6/150.0 MB 7.2 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 84.1/150.0 MB 7.2 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 85.7/150.0 MB 7.2 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 87.3/150.0 MB 7.2 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 88.9/150.0 MB 7.2 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 90.4/150.0 MB 7.2 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 92.0/150.0 MB 7.2 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 93.6/150.0 MB 7.2 MB/s eta 0:00:08\n",
      "   ------------------------- -------------- 95.2/150.0 MB 7.2 MB/s eta 0:00:08\n",
      "   ------------------------- -------------- 96.7/150.0 MB 7.2 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 98.3/150.0 MB 7.2 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 99.9/150.0 MB 7.2 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 101.4/150.0 MB 7.2 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 103.0/150.0 MB 7.2 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 104.6/150.0 MB 7.2 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 106.2/150.0 MB 7.2 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 107.7/150.0 MB 7.2 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 109.1/150.0 MB 7.2 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 110.9/150.0 MB 7.2 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 112.2/150.0 MB 7.2 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 113.8/150.0 MB 7.2 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 115.3/150.0 MB 7.2 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 116.7/150.0 MB 7.2 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 118.2/150.0 MB 7.2 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 119.8/150.0 MB 7.2 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 121.4/150.0 MB 7.2 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 122.9/150.0 MB 7.2 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 124.5/150.0 MB 7.2 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 125.8/150.0 MB 7.2 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 127.4/150.0 MB 7.2 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 129.0/150.0 MB 7.2 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 130.5/150.0 MB 7.2 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 132.1/150.0 MB 7.2 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 133.7/150.0 MB 7.2 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 135.3/150.0 MB 7.2 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 136.6/150.0 MB 7.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 138.1/150.0 MB 7.2 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 139.7/150.0 MB 7.2 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 141.3/150.0 MB 7.2 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 142.9/150.0 MB 7.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 144.4/150.0 MB 7.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 145.8/150.0 MB 7.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  147.3/150.0 MB 7.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  148.9/150.0 MB 7.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  149.9/150.0 MB 7.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  149.9/150.0 MB 7.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  149.9/150.0 MB 7.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  149.9/150.0 MB 7.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 150.0/150.0 MB 6.9 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-3.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49d3eaa7-f40d-4f87-ac48-8ae7a7437907",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/27 11:12:51 INFO mlflow.tracking.fluent: Experiment with name 'InsurancePremiumPrediction_XGBoost' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/27 11:29:42 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/07/27 11:29:55 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model Evaluation Results:\n",
      "RMSE: 928.14\n",
      "MAE: 624.06\n",
      "R² Score: -0.1528\n",
      "RMSLE: 1.0566\n",
      "📄 submission.csv with predictions saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# 📘 Insurance Premium Prediction – XGBoost Version\n",
    "# =============================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load training dataset\n",
    "df = pd.read_csv(\"C:/Users/Lenovo/Documents/Guvi/Project_3/train.csv\")\n",
    "\n",
    "# Drop unstructured columns\n",
    "df = df.drop(columns=[\"Policy Start Date\", \"Customer Feedback\"])\n",
    "\n",
    "# Feature lists\n",
    "numerical_cols = [\n",
    "    \"Age\", \"Annual Income\", \"Number of Dependents\", \"Health Score\",\n",
    "    \"Previous Claims\", \"Vehicle Age\", \"Credit Score\", \"Insurance Duration\"\n",
    "]\n",
    "\n",
    "categorical_cols = [\n",
    "    \"Gender\", \"Marital Status\", \"Education Level\", \"Occupation\",\n",
    "    \"Location\", \"Policy Type\", \"Smoking Status\",\n",
    "    \"Exercise Frequency\", \"Property Type\"\n",
    "]\n",
    "\n",
    "# Create new features (optional but recommended)\n",
    "df[\"Income_per_Dependent\"] = df[\"Annual Income\"] / (df[\"Number of Dependents\"] + 1)\n",
    "df[\"Claims_per_Year\"] = df[\"Previous Claims\"] / df[\"Insurance Duration\"].replace(0, 1)\n",
    "numerical_cols += [\"Income_per_Dependent\", \"Claims_per_Year\"]\n",
    "\n",
    "# Target variable\n",
    "target_col = \"Premium Amount\"\n",
    "X = df[numerical_cols + categorical_cols]\n",
    "y = np.log1p(df[target_col])  # Apply log transformation\n",
    "\n",
    "# Preprocessing pipelines\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"num\", numerical_transformer, numerical_cols),\n",
    "    (\"cat\", categorical_transformer, categorical_cols)\n",
    "])\n",
    "\n",
    "# Train/validation split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ML pipeline with placeholder model\n",
    "xgb_pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", XGBRegressor(objective=\"reg:squarederror\", random_state=42))\n",
    "])\n",
    "\n",
    "# Hyperparameter tuning\n",
    "param_dist = {\n",
    "    \"regressor__n_estimators\": [100, 200, 300],\n",
    "    \"regressor__learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"regressor__max_depth\": [3, 5, 7],\n",
    "    \"regressor__subsample\": [0.6, 0.8, 1.0],\n",
    "    \"regressor__colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV setup\n",
    "search = RandomizedSearchCV(\n",
    "    xgb_pipeline,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    scoring=\"r2\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Start MLflow experiment\n",
    "mlflow.set_experiment(\"InsurancePremiumPrediction_XGBoost\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "\n",
    "    # Train + tune\n",
    "    search.fit(X_train, y_train)\n",
    "    best_model = search.best_estimator_\n",
    "\n",
    "    # Predict on validation\n",
    "    y_pred_log = best_model.predict(X_val)\n",
    "    y_pred = np.expm1(y_pred_log)  # Inverse log1p\n",
    "    y_val_original = np.expm1(y_val)  # Inverse ground truth\n",
    "\n",
    "    # Metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_val_original, y_pred))\n",
    "    mae = mean_absolute_error(y_val_original, y_pred)\n",
    "    r2 = r2_score(y_val_original, y_pred)\n",
    "    rmsle = np.sqrt(mean_squared_error(np.log1p(y_val_original), np.log1p(np.maximum(y_pred, 0))))\n",
    "\n",
    "    # Log results\n",
    "    mlflow.log_params(search.best_params_)\n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "    mlflow.log_metric(\"MAE\", mae)\n",
    "    mlflow.log_metric(\"R2\", r2)\n",
    "    mlflow.log_metric(\"RMSLE\", rmsle)\n",
    "\n",
    "    # Save model\n",
    "    joblib.dump(best_model, \"best_xgb_model.pkl\")\n",
    "    mlflow.sklearn.log_model(best_model, \"xgb_model\")\n",
    "\n",
    "# Output\n",
    "print(\"✅ Model Evaluation Results:\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(f\"RMSLE: {rmsle:.4f}\")\n",
    "\n",
    "# ===============================\n",
    "# 📦 Predict on Unlabeled Test Set\n",
    "# ===============================\n",
    "test_unlabeled = pd.read_csv(\"C:/Users/Lenovo/Documents/Guvi/Project_3/test.csv\")\n",
    "test_unlabeled = test_unlabeled.drop(columns=[\"Policy Start Date\", \"Customer Feedback\"])\n",
    "\n",
    "# Add same features\n",
    "test_unlabeled[\"Income_per_Dependent\"] = test_unlabeled[\"Annual Income\"] / (test_unlabeled[\"Number of Dependents\"] + 1)\n",
    "test_unlabeled[\"Claims_per_Year\"] = test_unlabeled[\"Previous Claims\"] / test_unlabeled[\"Insurance Duration\"].replace(0, 1)\n",
    "\n",
    "# Predict\n",
    "X_unlabeled = test_unlabeled[numerical_cols + categorical_cols]\n",
    "test_log_preds = best_model.predict(X_unlabeled)\n",
    "test_preds = np.expm1(test_log_preds)  # Inverse transform\n",
    "\n",
    "# Save predictions\n",
    "submission = pd.DataFrame({\n",
    "    \"Customer_ID\": test_unlabeled.get(\"Customer_ID\", range(len(test_unlabeled))),\n",
    "    \"Predicted_Premium_Amount\": test_preds\n",
    "})\n",
    "submission.to_csv(\"C:/Users/Lenovo/Documents/Guvi/Project_3/submission_XGBoost.csv\", index=False)\n",
    "\n",
    "print(\"📄 submission.csv with predictions saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f3455e2-ad70-4ffd-af11-256609908645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"C:/Users/Lenovo/Documents/Guvi/Project_3/train.csv\")\n",
    "df = df.drop(columns=[\"Policy Start Date\", \"Customer Feedback\"])\n",
    "\n",
    "# Define features\n",
    "numerical_cols = [\n",
    "    \"Age\", \"Annual Income\", \"Number of Dependents\", \"Health Score\",\n",
    "    \"Previous Claims\", \"Vehicle Age\", \"Credit Score\", \"Insurance Duration\"\n",
    "]\n",
    "\n",
    "categorical_cols = [\n",
    "    \"Gender\", \"Marital Status\", \"Education Level\", \"Occupation\",\n",
    "    \"Location\", \"Policy Type\", \"Smoking Status\",\n",
    "    \"Exercise Frequency\", \"Property Type\"\n",
    "]\n",
    "\n",
    "# Feature engineering\n",
    "df[\"Income_per_Dependent\"] = df[\"Annual Income\"] / (df[\"Number of Dependents\"] + 1)\n",
    "df[\"Claims_per_Year\"] = df[\"Previous Claims\"] / df[\"Insurance Duration\"].replace(0, np.nan)\n",
    "df[\"Claims_per_Year\"] = df[\"Claims_per_Year\"].fillna(0)\n",
    "numerical_cols += [\"Income_per_Dependent\", \"Claims_per_Year\"]\n",
    "\n",
    "# Log transform skewed features\n",
    "log_transform_features = [\"Annual Income\", \"Credit Score\", \"Previous Claims\"]\n",
    "for col in log_transform_features:\n",
    "    df[col] = np.log1p(df[col])\n",
    "\n",
    "# Prepare target\n",
    "target_col = \"Premium Amount\"\n",
    "X = df[numerical_cols + categorical_cols]\n",
    "y = np.log1p(df[target_col])\n",
    "\n",
    "# Preprocessing\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"num\", numerical_transformer, numerical_cols),\n",
    "    (\"cat\", categorical_transformer, categorical_cols)\n",
    "])\n",
    "\n",
    "# Train/val split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Model pipeline\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Smaller randomized parameter grid\n",
    "param_dist = {\n",
    "    \"regressor__n_estimators\": [100, 150],\n",
    "    \"regressor__max_depth\": [10, 15, 20],\n",
    "    \"regressor__min_samples_split\": [2, 5],\n",
    "    \"regressor__max_features\": [\"sqrt\", \"log2\"]\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV\n",
    "search = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=8,\n",
    "    cv=3,\n",
    "    scoring=\"r2\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "search.fit(X_train, y_train)\n",
    "best_model = search.best_estimator_\n",
    "\n",
    "# MLflow tracking\n",
    "mlflow.set_experiment(\"InsurancePremiumPrediction_RF_Fast\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    y_pred_log = best_model.predict(X_val)\n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "    y_val_original = np.expm1(y_val)\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_val_original, y_pred))\n",
    "    mae = mean_absolute_error(y_val_original, y_pred)\n",
    "    r2 = r2_score(y_val_original, y_pred)\n",
    "    rmsle = np.sqrt(mean_squared_error(np.log1p(y_val_original), np.log1p(np.maximum(y_pred, 0))))\n",
    "\n",
    "    mlflow.log_params(search.best_params_)\n",
    "    mlflow.log_param(\"model_type\", \"RandomForest_RandomizedSearch\")\n",
    "    mlflow.log_metric(\"RMSE\", rmse)\n",
    "    mlflow.log_metric(\"MAE\", mae)\n",
    "    mlflow.log_metric(\"R2\", r2)\n",
    "    mlflow.log_metric(\"RMSLE\", rmsle)\n",
    "\n",
    "    signature = infer_signature(X_train, best_model.predict(X_train))\n",
    "    mlflow.sklearn.log_model(best_model, name=\"rf_model_fast\", signature=signature, input_example=X_train.iloc[:5])\n",
    "\n",
    "    joblib.dump(best_model, \"rf_model_fast.pkl\")\n",
    "\n",
    "# Print results\n",
    "print(\"✅ FAST Random Forest Evaluation Results:\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"R² Score: {r2:.4f}\")\n",
    "print(f\"RMSLE: {rmsle:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893b07b4-cbc4-4d5b-b532-14e7e1ff9004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
